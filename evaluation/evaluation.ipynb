{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25e5cf8c-8032-4bf2-b701-e9fb22b053d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2651671034.py, line 66)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[10], line 66\u001b[0;36m\u001b[0m\n\u001b[0;31m    eval_prompt =\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import json \n",
    "\n",
    "base_model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,  # Llama 2 7b, same as before\n",
    "    quantization_config=bnb_config,  # Same quantization config as before\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    add_bos_token=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"/home/featurize/work/TinyLLaMA/src/outputs/tinyllama-finetune-2024-04-16-04-06/checkpoint-1000\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "with open(\"/home/featurize/work/TinyLLaMA/src/data/query_test.json\", \"r\") as f:\n",
    "    query = json.load(f)\n",
    "    \n",
    "eval_prompt = f\"\"\" You are an expert at extracting useful information from user queries. I need you to extract meta information from the user's query.  The extraction reults contain 'year', 'month', 'day', 'file content', 'file type' information for file retriever to locate the file. The extracted information should exclusively contain key-value pairs. Additionally, please generate 5 synonyms for the extracted 'file content'. Below are 5 examples that meet these requirements:\n",
    "Example1\n",
    "### query: Project documentation from January 15, 2024, to February 20, 2024\n",
    "### information: {{'year': [2024, 2024], 'month': [1, 2], 'day': [15, 20], 'file content': ['Project Documentation', 'Project Files', 'Project Overview', 'Project Details', 'Project Progress Documentation'], 'file type': ['pdf', 'doc', 'docx']}}\n",
    "\n",
    "Example2\n",
    "### query: Find my photos from New York last summer\n",
    "### information: {{'year': [-1, -1], 'month': [6, 8], 'day': [0, 0], 'file content': ['Photo taken in New York', 'New York Image', 'New York Snapshot', 'New York Picture', 'New York Photograph'], 'file type': ['jpg', 'jpeg', 'png', 'heif', 'tiff']}}\n",
    "\n",
    "Example3\n",
    "### query: How is AI transforming healthcare diagnostics?\n",
    "### information: {{'year': [], 'month': [], 'day': [], 'file content': ['AI in Healthcare Diagnostics', 'Artificial Intelligence and Medical Imaging', 'Machine Learning for Early Detection', 'AI Applications in Healthcare', 'Innovations in AI-based Diagnostics'], 'file type': ['pdf', 'docx', 'pptx', 'mp4', 'mp3']}}\n",
    "\n",
    "Example4\n",
    "### query: Conference materials from the Global Tech Summit held from 2023/10/10 to 2023/10/12\n",
    "### information: {{'year': [2023, 2023], 'month': [10, 10], 'day': [10, 12], 'file content' : ['Global Tech Summit Materials', 'Tech Summit Presentations', 'Tech Conference Docs', 'Tech Summit Slides', 'Tech Summit Proceedings'], 'file type': ['pdf', 'pptx', 'doc', 'docx']}}\n",
    "\n",
    "Example5\n",
    "### query: The best ways to introduce coding to children\n",
    "### information: {{'year': [], 'month': [], 'day': [], 'file content': ['Coding for Kids', 'Children\\'s Programming Basics', 'Fun Coding Projects for Kids', 'Learning to Code Through Games', 'Introduction to Programming for Young Learners'], 'file type': ['pdf', 'docx', 'pptx', 'mp4']}}\n",
    "\n",
    "Now, please extract meta information from this user query:\n",
    "### query: {\"photos do not contain cats\"}\n",
    "### information: \"\"\"    \n",
    "#     return full_prompt\n",
    "\n",
    "\n",
    "# {query[4][\"input\"]}\n",
    "\n",
    "eval_prompt = \"\"\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=150)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52fa171-5ce8-4a95-b535-62a11ae7d783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pprint import pprint\n",
    "import ast\n",
    "\n",
    "def extract_answer(prediction):\n",
    "\n",
    "    # Extracting the part of the text after the specified prompt\n",
    "    relevant_part = prediction.split(\"Now, please extract meta information from this user query:\")[-1].strip()\n",
    "\n",
    "    # Updated regex pattern to accommodate potential variations in formatting\n",
    "    pattern = r\"### query:\\s*(.*?)\\n### information:\\s*(\\{.*?\\})\\s*\"\n",
    "\n",
    "    # Search for the pattern in the relevant part of the text\n",
    "    match = re.search(patt0ern, relevant_part, re.DOTALL)\n",
    "\n",
    "    extracted_information = {}\n",
    "\n",
    "    if match:\n",
    "        # Extract query and information string\n",
    "        query, information_str = match.groups()\n",
    "        # Convert information string into a Python dictionary\n",
    "        information = ast.literal_eval(information_str)\n",
    "        extracted_information[\"query\"] = query\n",
    "        extracted_information[\"information\"] = information\n",
    "    return extracted_information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf39578-a9d6-4e8b-a7a8-950bd6e11dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env OPENAI_API_KEY= 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\n",
    "\n",
    "def evaluate_by_gpt(truth, pred):\n",
    "    pred = str(extract_answer(pred)[\"information\"])\n",
    "    truth = str(truth[\"information\"])\n",
    "    \n",
    "    prompt = \"\"\" \n",
    "    You are an expert at evaluating LLMs predictions. The output contains 'year', 'month', 'day', 'file content', 'file type' information.\n",
    "    'year', 'month', 'day' should be exactly the same. For 'file content' and 'file type', you can just qualitatively evaluate it by measuring the\n",
    "    semantic similarty without being too strict. Based on the above evaluation metric, if you think the prediction is good, return 1, otherwise, return 0.\n",
    "    your response should be only one numer: 0 or 1. Here is the ground truth label and prediction results.\n",
    "    \n",
    "    ### Ground Truth: {truth}\n",
    "    ### Prediction: {pred}\n",
    "        \"\"\"\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4', #gpt-4\n",
    "        messages=[\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': [\n",
    "                    {'type': 'text', 'text': prompt},\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=2000,\n",
    "    )\n",
    "    query_tasks = response.choices[0].message.content\n",
    "    return query_tasks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ed35bfd-85c3-4ffa-b136-1b26e12a9f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'information': {'day': [15, 20],\n",
      "                 'file content': ['Recommendation Engine',\n",
      "                                  'Recommendation System',\n",
      "                                  'Recommendation Algorithm',\n",
      "                                  'Recommendation System',\n",
      "                                  'Recommendation Engine'],\n",
      "                 'file type': ['pdf', 'doc', 'docx', 'txt', 'md'],\n",
      "                 'month': [1, 2],\n",
      "                 'year': [2024, 2024]},\n",
      " 'query': 'How to use LLMs or leverage them for recommendation engine'}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pprint import pprint\n",
    "import ast\n",
    "\n",
    "# Input text, potentially long, containing various sections\n",
    "input_text = \"\"\"\n",
    "Example1\n",
    "### query: Project documentation from January 15, 2024, to February 20, 2024\n",
    "### information: {'year': [2024, 2024], 'month': [1, 2], 'day': [15, 20], 'file content': ['Project Documentation', 'Project Files', 'Project Overview', 'Project Details', 'Project Progress Documentation'], 'file type': ['pdf', 'doc', 'docx']}\n",
    "\n",
    "Now, please extract meta information from this user query:\n",
    "### query: How to use LLMs or leverage them for recommendation engine\n",
    "### information:  {'year': [2024, 2024], 'month': [1, 2], 'day': [15, 20], 'file content': ['Recommendation Engine', 'Recommendation System', 'Recommendation Algorithm', 'Recommendation System', 'Recommendation Engine'], 'file type': ['pdf', 'doc', 'docx', 'txt', 'md']}\n",
    "\n",
    "### query: Find my photos from New York last summer\n",
    "### information: {'year': [-1, -1], 'month': [6, 8], 'day': [0, 0], 'file content': ['Photo taken in New\n",
    "\"\"\"\n",
    "\n",
    "# Isolate the portion of the text after the specific prompt\n",
    "relevant_part = input_text.split(\"Now, please extract meta information from this user query:\")[-1].strip()\n",
    "\n",
    "# Regular expression to match the query and its corresponding information\n",
    "pattern = r\"### query:\\s*(.*?)\\n### information:\\s*(\\{.*?\\})\\s*\"\n",
    "\n",
    "# Find the first match in the relevant part of the text\n",
    "match = re.search(pattern, relevant_part, re.DOTALL)\n",
    "\n",
    "# Initialize result dictionary\n",
    "extracted_information = {}\n",
    "\n",
    "if match:\n",
    "    query, information_str = match.groups()\n",
    "    # Safely evaluate the information string into a Python dictionary\n",
    "    information = ast.literal_eval(information_str)\n",
    "    extracted_information[\"query\"] = query\n",
    "    extracted_information[\"information\"] = information\n",
    "\n",
    "# Display the extracted information\n",
    "pprint(extracted_information)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "effc470b-160c-4918-be58-cff510ceecb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'information': {'day': [15, 20],\n",
      "                 'file content': ['Recommendation Engine',\n",
      "                                  'Recommendation System',\n",
      "                                  'Recommendation Algorithm',\n",
      "                                  'Recommendation System',\n",
      "                                  'Recommendation Engine'],\n",
      "                 'file type': ['pdf', 'doc', 'docx', 'txt', 'md'],\n",
      "                 'month': [1, 2],\n",
      "                 'year': [2024, 2024]},\n",
      " 'query': 'How to use LLMs or leverage them for recommendation engine'}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pprint import pprint\n",
    "import ast\n",
    "\n",
    "# Updated input text as an example\n",
    "input_text = \"\"\"\n",
    "Example1\n",
    "### query: Project documentation from January 15, 2024, to February 20, 2024\n",
    "### information: {'year': [2024, 2024], 'month': [1, 2], 'day': [15, 20], 'file content': ['Project Documentation', 'Project Files', 'Project Overview', 'Project Details', 'Project Progress Documentation'], 'file type': ['pdf', 'doc', 'docx']}\n",
    "\n",
    "Now, please extract meta information from this user query:\n",
    "### query: How to use LLMs or leverage them for recommendation engine\n",
    "### information:  {'year': [2024, 2024], 'month': [1, 2], 'day': [15, 20], 'file content': ['Recommendation Engine', 'Recommendation System', 'Recommendation Algorithm', 'Recommendation System', 'Recommendation Engine'], 'file type': ['pdf', 'doc', 'docx', 'txt', 'md']}\n",
    "\n",
    "### query: Find my photos from New York last summer\n",
    "### information: {'year': [-1, -1], 'month': [6, 8], 'day': [0, 0], 'file content': ['Photo taken in New\n",
    "\"\"\"\n",
    "\n",
    "# Extracting the part of the text after the specified prompt\n",
    "relevant_part = input_text.split(\"Now, please extract meta information from this user query:\")[-1].strip()\n",
    "\n",
    "# Updated regex pattern to accommodate potential variations in formatting\n",
    "pattern = r\"### query:\\s*(.*?)\\n### information:\\s*(\\{.*?\\})\\s*\"\n",
    "\n",
    "# Search for the pattern in the relevant part of the text\n",
    "match = re.search(pattern, relevant_part, re.DOTALL)\n",
    "\n",
    "extracted_information = {}\n",
    "\n",
    "if match:\n",
    "    # Extract query and information string\n",
    "    query, information_str = match.groups()\n",
    "    # Convert information string into a Python dictionary\n",
    "    information = ast.literal_eval(information_str)\n",
    "    extracted_information[\"query\"] = query\n",
    "    extracted_information[\"information\"] = information\n",
    "\n",
    "# Print the extracted information\n",
    "pprint(extracted_information)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da24d617-32a0-4d48-b1be-029f34e24c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'information': {'day': [15, 20],\n",
      "                 'file content': ['Recommendation Engine',\n",
      "                                  'Recommendation System',\n",
      "                                  'Recommendation Algorithm',\n",
      "                                  'Recommendation System',\n",
      "                                  'Recommendation Engine'],\n",
      "                 'file type': ['pdf', 'doc', 'docx', 'txt', 'md'],\n",
      "                 'month': [1, 2],\n",
      "                 'year': [2024, 2024]},\n",
      " 'query': 'How to use LLMs or leverage them for recommendation engine'}\n"
     ]
    }
   ],
   "source": [
    "input_text = eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=150)[0], skip_special_tokens=True)\n",
    "\n",
    "# Isolate the portion of the text after the specific prompt\n",
    "relevant_part = input_text.split(\"Now, please extract meta information from this user query:\")[-1].strip()\n",
    "\n",
    "# Regular expression to match the query and its corresponding information\n",
    "pattern = r\"### query:\\s*(.*?)\\n### information:\\s*(\\{.*?\\})\\s*\"\n",
    "\n",
    "# Find the first match in the relevant part of the text\n",
    "match = re.search(pattern, relevant_part, re.DOTALL)\n",
    "\n",
    "# Initialize result dictionary\n",
    "extracted_information = {}\n",
    "\n",
    "if match:\n",
    "    query, information_str = match.groups()\n",
    "    # Safely evaluate the information string into a Python dictionary\n",
    "    information = ast.literal_eval(information_str)\n",
    "    extracted_information[\"query\"] = query\n",
    "    extracted_information[\"information\"] = information\n",
    "\n",
    "# Display the extracted information\n",
    "pprint(extracted_information)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
